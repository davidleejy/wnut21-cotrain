{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('data_codeswitch': conda)"
  },
  "interpreter": {
   "hash": "e2bcd92a3bd8a5be133fd4431bd982d12b8f174f9073fbee03c1fd82f05176b0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "This notebook downloads code diffs and samples a desired number of negative commits from the RA21 dataset.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def df_to_csv(df, path):\n",
    "    print(f'Trying to save {len(df)} rows.')\n",
    "    df.to_csv(path, index = False, errors='replace')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def download_diff(owner, repo, hash, token=None):\n",
    "    # Downloads a commit's code diff using github's API.\n",
    "    # Example call: \n",
    "    #   download_diff(owner = \"TQRG\", repo = \"security-patches-dataset\", hash = \"6ee8095bb232d1e190bf453af09fc87cae558d02\")\n",
    "    #\n",
    "    # Returns a request object.\n",
    "    query_url = f\"https://api.github.com/repos/{owner}/{repo}/commits/{hash}\"\n",
    "    headers = {'Accept': 'application/vnd.github.v3.diff'}\n",
    "    if token:\n",
    "        headers['Authorization'] = f'token {token}'\n",
    "    return requests.get(query_url, headers=headers)\n",
    "\n",
    "# demo:\n",
    "r = download_diff(owner = \"TQRG\", repo = \"security-patches-dataset\", hash = \"6ee8095bb232d1e190bf453af09fc87cae558d02\")\n",
    "r.text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def download_diffs_all_positive(df, rate_limit_per_sec, token=None, report_every=100, rate_limit_leeway=0.05):\n",
    "    # Downloads all code diffs for the positive dataset.\n",
    "    # Code diffs are written under column \"diff\" of `df`.\n",
    "    #\n",
    "    # Inputs:\n",
    "    #   `df`: Pandas dataframe of the positive sample dataset positive.csv\n",
    "    #   `rate_limit_per_sec`: Github API's rate limit. (units: per second). For example\n",
    "    #       5000 requests per hour == 5000/60/60 per second.\n",
    "    #   `token`: Your Github API token. Optional.\n",
    "    #   `report_every`: Prints an update after working thru every `report_every` rows. Default 100 rows.\n",
    "    #   `rate_limit_leeway`: Margin of safety on the rate limit to avoid accidentally triggering it. Default 5%.\n",
    "    #\n",
    "    # Returns:\n",
    "    #   - A pandas dataframe containing the code diffs column\n",
    "    #   - An error List.\n",
    "    #\n",
    "\n",
    "    errors = []\n",
    "    df['diff'] = np.nan # create empty column to write code diffs to.\n",
    "    leeway = 1 + rate_limit_leeway\n",
    "    duration_per_request_minimum_s = (1.0 / rate_limit_per_sec) * leeway\n",
    "\n",
    "    for i in df.index:\n",
    "        start = time.time()\n",
    "        skip = False\n",
    "\n",
    "        hash = df['sha'][i]\n",
    "        project = df['project'][i]\n",
    "        if 'github' not in project.lower():\n",
    "            print(f'[WARN] {project} (row {i}) not on Github. Cannot use Github API. Skipped.')\n",
    "            errors.append(f'row indexed {i} not-github-project')\n",
    "            skip = True   #skip row\n",
    "        \n",
    "        split = project.split('/')\n",
    "        owner, repo = split[-2], split[-1]\n",
    "        r = download_diff(owner, repo, hash, token)\n",
    "        if 200 != r.status_code:\n",
    "            print(f'[WARN] GET request for {project} (row {i}) failed with status code {r.status_code}. Skipped.')\n",
    "            errors.append(f'row indexed {i} status-not-200')\n",
    "            skip = True   #skip row\n",
    "\n",
    "        if not skip:\n",
    "            df['diff'][i] = r.text\n",
    "\n",
    "        if (i+1) % report_every == 0:\n",
    "            print(f'[INFO] Done {i+1} rows.')\n",
    "\n",
    "        duration = time.time() - start\n",
    "        if duration < duration_per_request_minimum_s:\n",
    "            sleep_s = duration_per_request_minimum_s - duration\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "    if errors:\n",
    "        print('[WARN] ERRORS DETECTED. SEE VARIABLE `errors` FOR DETAILS.')\n",
    "    else:\n",
    "        print('[INFO] No errors.')\n",
    "\n",
    "    return df, errors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download all code diffs for positive dataset.\n",
    "\n",
    "df = pd.read_csv('security-patches-dataset/dataset/positive.csv')\n",
    "token = 'nfhrgioKJNIOEng358943'   # Your github token goes here.\n",
    "rate_limit_per_sec = 5000 / 60 / 60     # Github API's rate limit for basic (non-enterprise) users.\n",
    "df, errors = download_diffs_all_positive(df, rate_limit_per_sec=rate_limit_per_sec, token=token, report_every=100)\n",
    "display(df)\n",
    "df_to_csv(df, path='positive+CC.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter positive & negative dataset for projects that appear in both sets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get projects in positive set.\n",
    "\n",
    "dfp = pd.read_csv('positive+CC.csv')\n",
    "dfp = dfp.dropna(subset=['diff'])\n",
    "# print(dfp)\n",
    "projects = dfp['project'].str.lower().unique()\n",
    "projects = [p[p.find('github.com'): ] for p in projects] # remove \"https://www.\" part\n",
    "print(projects[:10], len(projects))\n",
    "\n",
    "# Get porjects in negative set.\n",
    "\n",
    "dfn = pd.read_csv('security-patches-dataset/dataset/negative.csv')\n",
    "projects_n = dfn['github'].str.lower()\n",
    "projects_n = [p[p.find('github.com'): ] for p in projects_n] # remove \"https://www.\" part\n",
    "projects_n = [p[ : p.rfind('/commit') ] for p in projects_n] # remove \"/commit/0d533c3615f7c54fa2b64d16\" part\n",
    "projects_n = list(set(projects_n)) # unique-fy\n",
    "print(projects_n[:10], len(projects_n))\n",
    "\n",
    "# Find overlap between positive and negative sets.\n",
    "\n",
    "overlap = set(projects) & set(projects_n)\n",
    "missing_in_n = len(set(projects) - set(projects_n))\n",
    "missing_in_p = len(set(projects_n) - set(projects))\n",
    "print(f'{len(overlap) = } {missing_in_n = } {missing_in_p = }')\n",
    "\n",
    "# Will only work on projects present in both pos & neg sets.\n",
    "\n",
    "keep_list = [re.escape(project) for project in overlap]\n",
    "dfp_new = dfp[dfp['project'].str.contains('|'.join(keep_list), regex=True, flags=re.IGNORECASE)]\n",
    "dfn_new = dfn[dfn['github'].str.contains('|'.join(keep_list), regex=True, flags=re.IGNORECASE)]\n",
    "print(f'{len(dfp_new) = } {len(dfn_new) = }')\n",
    "\n",
    "# Save new sets.\n",
    "\n",
    "df_to_csv(dfp_new, path=f'positive+CC-{len(overlap)}repos.csv')\n",
    "df_to_csv(dfn_new, path=f'negative-{len(overlap)}repos.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download code changes for chosen negative samples\n",
    "\n",
    "Two options:\n",
    "\n",
    "1. Sample negative commits constrained to repositories present in both positive & negative data.\n",
    "\n",
    "2. Sample negative commits without constrain."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# OPTION 1: \n",
    "# Sample negative commits constrained to repositories present in both positive & negative data. \n",
    "\n",
    "\n",
    "# Set number of negative samples N to download code changes for:\n",
    "dfp = pd.read_csv('positive+CC-900repos.csv')\n",
    "N = len(dfp)\n",
    "# N = 2 * len(dfp)   # Example: sample 2x as many.\n",
    "print(f'{N = }')\n",
    "\n",
    "# Read in negative dataset:\n",
    "dfn = pd.read_csv('negative-900repos.csv')\n",
    "projects_n = dfn['github'].str.lower()\n",
    "projects_n = [p[p.find('github.com'): ] for p in projects_n] # remove \"https://www.\" part\n",
    "projects_n = [p[ : p.rfind('commit') ] for p in projects_n] # remove \"commit/0d533c3615f7c54fa2b64d16\" part\n",
    "projects_n = set(projects_n) # unique-fy\n",
    "print(f'{len(projects_n) = }')\n",
    "\n",
    "# Sample negative commits.\n",
    "\n",
    "dfn['select'] = np.nan # make empty column.\n",
    "extra = 2   # extra samples in case some commits don't have code diffs available.\n",
    "samples_per_project = math.ceil(N / len(projects_n)) + extra\n",
    "\n",
    "for i, project in enumerate(projects_n):\n",
    "    df = dfn[dfn['github'].str.contains(re.escape(project), regex=True, flags=re.IGNORECASE)] # get rows of this project\n",
    "    if len(df) == 0:\n",
    "        print(f'[WARN] {project =} has no rows.')\n",
    "    seed = i\n",
    "    idxs_select = df.sample(n=min(samples_per_project, len(df)), random_state=seed).index\n",
    "    dfn['select'][idxs_select] = 'A'   # 'A' denotes selected.\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# OPTION 2: \n",
    "# Sample negative commits without constrain.\n",
    "\n",
    "# Set number of negative samples N to download code changes for:\n",
    "dfp = pd.read_csv('positive+CC.csv')\n",
    "# N = len(dfp)\n",
    "N = 2 * len(dfp)\n",
    "print(f'{N = }')\n",
    "\n",
    "# Read in negative dataset:\n",
    "dfn = pd.read_csv('security-patches-dataset/dataset/negative.csv')\n",
    "projects_n = dfn['github'].str.lower()\n",
    "projects_n = [p[p.find('github.com'): ] for p in projects_n] # remove \"https://www.\" part\n",
    "projects_n = [p[ : p.rfind('commit') ] for p in projects_n] # remove \"commit/0d533c3615f7c54fa2b64d16\" part\n",
    "projects_n = set(projects_n) # unique-fy\n",
    "print(f'{len(projects_n) = }')\n",
    "\n",
    "# Sample negative commits.\n",
    "\n",
    "dfn['select'] = np.nan # make empty column.\n",
    "extra = 1   # extra samples in case some commits don't have code diffs available.\n",
    "samples_per_project = math.ceil(N / len(projects_n)) + extra\n",
    "\n",
    "for i, project in enumerate(projects_n):\n",
    "    df = dfn[dfn['github'].str.contains(re.escape(project), regex=True, flags=re.IGNORECASE)] # get rows of this project\n",
    "    if len(df) == 0:\n",
    "        print(f'[WARN] {project =} has no rows.')\n",
    "    seed = i + 100\n",
    "    # print(df)\n",
    "    idxs_select = df.sample(n=min(samples_per_project, len(df)), random_state=seed).index\n",
    "    # print(idxs_select)\n",
    "    dfn['select'][idxs_select] = 'A'    # 'A' denotes selected."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# View selected negative commits.\n",
    "\n",
    "def view(dfn):\n",
    "    dfn_select = dfn[dfn['select'] == 'A']\n",
    "    pp = dfn_select['github'].str.lower()\n",
    "    pp = [p[p.find('github.com'): ] for p in pp] # remove \"https://www.\" part\n",
    "    pp = [p[ : p.rfind('commit') ] for p in pp] # remove \"commit/0d533c3615f7c54fa2b64d16\" part\n",
    "    pp = list(set(pp)) # unique-fy\n",
    "\n",
    "    print(f'num projects: {len(pp)}. num rows {len(dfn_select) =}')\n",
    "\n",
    "    display(dfn_select)\n",
    "\n",
    "view(dfn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def download_diffs_for_negative_dataset(df, \n",
    "                        duration_per_request_minimum_s,\n",
    "                        token=None, \n",
    "                        report_every=100):\n",
    "\n",
    "    errors = []\n",
    "    n_done = 0\n",
    "\n",
    "    for i in df.index:\n",
    "        start = time.time()\n",
    "        skip = False\n",
    "        \n",
    "        url = df['github'][i]\n",
    "        _,_,_,owner,repo,_,hash = url.split('/')\n",
    "\n",
    "        if 'github' not in url.lower():\n",
    "            print(f'[WARN] {url} (row indexed {i}) not on Github. Cannot use Github API. Skipped.')\n",
    "            errors.append(f'row {i} not-github-project')\n",
    "            skip = True  # skip row\n",
    "\n",
    "        r = download_diff(owner, repo, hash, token)\n",
    "        \n",
    "        if 200 != r.status_code:\n",
    "            print(f'[WARN] GET request for {url} (row indexed {i}) failed with status code {r.status_code}. Skipped.')\n",
    "            errors.append(f'row indexed {i} status-not-200')\n",
    "            skip = True  # skip row\n",
    "\n",
    "        if not skip:\n",
    "            df['diff'][i] = r.text\n",
    "\n",
    "        n_done += 1\n",
    "        if 0 == n_done % report_every:\n",
    "            print(f'[INFO] Done {n_done} rows.')\n",
    "\n",
    "        duration = time.time() - start\n",
    "        if duration < duration_per_request_minimum_s:\n",
    "            sleep_s = duration_per_request_minimum_s - duration\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "    if errors:\n",
    "        print('[WARN] ERRORS DETECTED. SEE VARIABLE `errors` FOR DETAILS.')\n",
    "    else:\n",
    "        print('[INFO] No errors.')\n",
    "\n",
    "    return df, errors\n",
    "\n",
    "token = 'ajksdgfuiy89y4tbr9g984'   # your github token goes here.\n",
    "rate_limit_per_sec = (5000 - 1) / 60 / 60     # Github API's rate limit for basic (non-enterprise) users is 5000 requests per hour. -1 for leeway.\n",
    "duration_per_request_minimum_s = (1.0 / rate_limit_per_sec) * 1.07      # additional 7% time leeway.\n",
    "\n",
    "dfn_select = dfn[dfn['select'] == 'A']\n",
    "dfn_select['diff']  = np.nan # make empty column\n",
    "\n",
    "df_cc, errors = download_diffs_for_negative_dataset(dfn_select, duration_per_request_minimum_s=duration_per_request_minimum_s, token=token)\n",
    "\n",
    "display(df_cc)\n",
    "\n",
    "# Save df\n",
    "dt = '{:%Y-%m-%d-%H%M}'.format(datetime.now())\n",
    "df_to_csv(df_cc, path=f'negative-{dt}.csv')   #  filename like: negative-2017-06-27-1856.csv\n",
    "\n",
    "# If you had chosen Option 1, the saved file is conceptually similar to `negative+cc-900repos`"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Truncate code diffs that are too long if you'd like.\n",
    "\n",
    "df=pd.read_csv('negative-2021-09-10-1818.csv')\n",
    "print(max(df['diff'].str.len()))\n",
    "df['diff'] = df['diff'].str.slice(0, 200000)\n",
    "print(max(df['diff'].str.len()))\n",
    "dt = '{:%Y-%m-%d-%H%M}'.format(datetime.now())\n",
    "df_to_csv(df_cc, path='negative-%s.csv'%( '{:%Y-%m-%d-%H%M}'.format(datetime.now()) ))   #  filename like: negative-2017-06-27-1856.csv"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}